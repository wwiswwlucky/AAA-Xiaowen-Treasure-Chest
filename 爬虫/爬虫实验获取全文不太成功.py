import requests
from bs4 import BeautifulSoup
import re
import time
import pandas as pd

headers = {
    "User-Agent": "Mozilla/5.0"
}


# âœ… æå–å…±ç°å¥
def extract_sentences(text, keywords):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if all(k.lower() in s.lower() for k in keywords)]


# âœ… æŠ“å–æ‘˜è¦ + åˆ¤æ–­ PMC é“¾æ¥
def get_abstract(pmid):
    url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    # æŠ½å–æ‘˜è¦
    abstract_div = soup.find("div", class_="abstract-content")
    abstract = abstract_div.get_text(separator=" ", strip=True) if abstract_div else ""

    # æŠ½å– PMC é“¾æ¥
    pmc_link_tag = soup.select_one("a.link-item.pmc")
    pmc_url = ""
    if pmc_link_tag:
        href = pmc_link_tag.get("href", "")
        pmc_url = href if href.startswith("http") else "https://www.ncbi.nlm.nih.gov" + href

    return abstract, pmc_url


# âœ… è·å– PMC å…è´¹å…¨æ–‡å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼‰
def get_pmc_full_text(pmc_url):
    try:
        res = requests.get(pmc_url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        # ä¼˜å…ˆæŠ“ç»“æ„åŒ–çš„ <div class="tsec sec">
        sections = soup.find_all("div", class_="tsec sec")
        if sections:
            return "\n\n".join(sec.get_text(separator=" ", strip=True) for sec in sections)

        # å¦åˆ™æŠ“ä¸»å†…å®¹åŒº
        main = soup.find("div", id="main-content") or soup.find("article")
        return main.get_text(separator=" ", strip=True) if main else "æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹"
    except Exception as e:
        return f"å…¨æ–‡æŠ“å–å¤±è´¥: {e}"


# âœ… PubMed æœç´¢å¹¶å¤„ç†æ¯æ¡æ–‡çŒ®
def search_pubmed_with_fulltext(query, max_results=5):
    url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query.replace(' ', '+')}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')

    articles = soup.select(".docsum-content")[:max_results]
    results = []

    for art in articles:
        title_tag = art.select_one("a.docsum-title")
        if title_tag:
            title = title_tag.text.strip()
            href = title_tag.get("href", "")
            pmid = href.strip("/").split("/")[-1]

            # æŠ“æ‘˜è¦å’ŒPMCé“¾æ¥
            abstract, pmc_url = get_abstract(pmid)

            # å…±ç°åˆ¤æ–­
            keywords = re.findall(r'"([^"]+)"', query)
            co_sentences = extract_sentences(abstract, keywords)

            # è·å–å…¨æ–‡ï¼ˆå¦‚æœ‰ï¼‰
            full_text = get_pmc_full_text(pmc_url) if pmc_url else "æ— å…è´¹å…¨æ–‡"

            result = {
                "PMID": pmid,
                "æ ‡é¢˜": title,
                "æ‘˜è¦": abstract,
                "æ˜¯å¦å…±ç°": "æ˜¯" if co_sentences else "å¦",
                "å…±ç°å¥å­": co_sentences[0] if co_sentences else "",
                "æ˜¯å¦æœ‰å…è´¹å…¨æ–‡": "æ˜¯" if pmc_url else "å¦",
                "PMCé“¾æ¥": pmc_url,
                "å…¨æ–‡å†…å®¹é¢„è§ˆ": full_text[:500] if "æœªæ‰¾åˆ°" not in full_text else full_text
            }
            results.append(result)
            time.sleep(1)
    return results


# âœ… ç¤ºä¾‹è¿è¡Œ
query = '"Retinal atrophy" AND ABCA4'
results = search_pubmed_with_fulltext(query, max_results=5)

pd.DataFrame(results).to_excel("D:/Desktop/PubMed_å¢å¼ºç‰ˆå…¨æ–‡çˆ¬å–.xlsx", index=False)



# âœ… è¾“å‡ºç»“æœ
for r in results:
    print(f"\nğŸ“Œ PMID: {r['PMID']}")
    print(f"ğŸ”¹ æ ‡é¢˜: {r['æ ‡é¢˜']}")
    print(f"ğŸ“„ æ˜¯å¦å…±ç°: {r['æ˜¯å¦å…±ç°']}")
    print(f"ğŸ§  å…±ç°å¥å­: {r['å…±ç°å¥å­']}")
    print(f"ğŸ“‘ æ‘˜è¦: {r['æ‘˜è¦']}")
    print(f"ğŸ“– æ˜¯å¦æœ‰å…è´¹å…¨æ–‡: {r['æ˜¯å¦æœ‰å…è´¹å…¨æ–‡']}")
    print(f"ğŸ”— PMCé“¾æ¥: {r['PMCé“¾æ¥']}")
    print(f"ğŸ“„ å…¨æ–‡å†…å®¹é¢„è§ˆ:\n{r['å…¨æ–‡å†…å®¹é¢„è§ˆ']}\n")
    print("âœ… ç»“æœå·²ä¿å­˜åˆ° Excel")
